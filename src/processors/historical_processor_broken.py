"""
Historical Data Processor
Processes historical blockchain data in parallel batches.
"""

import asyncio
import logging
from typing import Optional, List, Dict, Any
from datetime import datetime, timezone
import json
from concurrent.futures import ThreadPoolExecutor, as_completed
from multiprocessing import Pool, cpu_count
import time

from core.blockchain_client import BlockchainClient
from core.influxdb_client import BlockchainInfluxDB
from core.config import Config
from rich.console import Console
from rich.progress import Progress, BarColumn, TextColumn, TimeRemainingColumn
from rich.table import Table
from rich.live import Live
import structlog

logger = structlog.get_logger(__name__)
console = Console()


class HistoricalProcessor:
    """Processes historical blockchain data with parallel batching."""
    
    def __init__(self, config: Config):
        self.config = config
        self.blockchain_client = BlockchainClient(config)
        
        # InfluxDB client (will be initialized when token is available)
        self.db_client = None
        if config.influxdb_token:
            self.db_client = BlockchainInfluxDB(config)
        
        # Processing state
        self.start_block = config.processing_start_block
        self.batch_size = config.processing_batch_size
        self.max_workers = config.processing_max_workers
        
        # Statistics
        self.stats = {
            'blocks_processed': 0,
            'transactions_processed': 0,
            'events_processed': 0,
            'errors': 0,
            'start_time': None,
            'blocks_per_second': 0.0
        }
        
    async def initialize(self) -> bool:
        """Initialize connections to blockchain and database."""
        logger.info("Initializing historical processor...")\n        
        # Connect to blockchain\n        blockchain_connected = await self.blockchain_client.connect()\n        if not blockchain_connected:\n            logger.error("Failed to connect to blockchain")\n            return False\n            \n        # Connect to database if available\n        db_connected = True\n        if self.db_client:\n            db_connected = await self.db_client.connect()\n            if not db_connected:\n                logger.warning("Database connection failed, will continue without storing data")\n                \n        logger.info(\n            "Initialization complete",\n            blockchain_connected=blockchain_connected,\n            database_connected=db_connected,\n            chain_id=self.blockchain_client.chain_id\n        )\n        \n        return True\n        \n    async def get_processing_range(self) -> tuple[int, int]:\n        """Determine the range of blocks to process."""\n        # Get latest block from blockchain\n        latest_blockchain_block = await self.blockchain_client.get_latest_block_number()\n        if latest_blockchain_block is None:\n            raise ValueError("Could not get latest block from blockchain")\n            \n        # Determine end block\n        if self.config.processing_end_block == "latest":\n            end_block = latest_blockchain_block - self.config.confirmation_blocks\n        else:\n            end_block = min(int(self.config.processing_end_block), latest_blockchain_block)\n            \n        # Check if we have any existing data in InfluxDB\n        start_block = self.start_block\n        if self.db_client:\n            try:\n                latest_db_block = self.db_client.query_latest_block()\n                if latest_db_block is not None:\n                    start_block = max(start_block, latest_db_block + 1)\n                    logger.info(f"Resuming from block {start_block} (database has up to {latest_db_block})")\n            except Exception as e:\n                logger.warning(f"Could not query latest block from database: {e}")\n                \n        return start_block, end_block\n        \n    async def process_block_batch(self, start_block: int, end_block: int) -> Dict[str, Any]:\n        """Process a batch of blocks."""\n        batch_stats = {\n            'blocks_processed': 0,\n            'transactions_processed': 0,\n            'events_processed': 0,\n            'errors': 0,\n            'batch_start': start_block,\n            'batch_end': end_block\n        }\n        \n        try:\n            # Get blocks in batch\n            blocks = await self.blockchain_client.get_blocks_batch(start_block, end_block)\n            \n            for i, block in enumerate(blocks):\n                block_number = start_block + i\n                \n                if isinstance(block, Exception):\n                    logger.error(f"Error getting block {block_number}: {block}")\n                    batch_stats['errors'] += 1\n                    continue\n                    \n                if block is None:\n                    logger.warning(f"Block {block_number} returned None")\n                    batch_stats['errors'] += 1\n                    continue\n                    \n                # Process the block\n                block_stats = await self.process_single_block(block, block_number)\n                batch_stats['blocks_processed'] += 1\n                batch_stats['transactions_processed'] += block_stats.get('transactions', 0)\n                batch_stats['events_processed'] += block_stats.get('events', 0)\n                \n        except Exception as e:\n            logger.error(f"Error processing batch {start_block}-{end_block}: {e}")\n            batch_stats['errors'] += 1\n            \n        return batch_stats\n        \n    async def process_single_block(self, block_data: Dict[str, Any], block_number: int) -> Dict[str, Any]:\n        """Process a single block and its transactions."""\n        block_stats = {'transactions': 0, 'events': 0}\n        \n        try:\n            # Calculate block time if we have previous block\n            block_time_diff = None\n            if block_number > 0:\n                try:\n                    prev_block = await self.blockchain_client.get_block(block_number - 1, False)\n                    if prev_block:\n                        curr_timestamp = int(block_data['timestamp'], 16)\n                        prev_timestamp = int(prev_block['timestamp'], 16)\n                        block_time_diff = curr_timestamp - prev_timestamp\n                except Exception as e:\n                    logger.debug(f"Could not calculate block time for {block_number}: {e}")\n            \n            # Store block data\n            if self.db_client:\n                self.db_client.write_block(block_data, block_time_diff)\n                \n            # Process transactions\n            transactions = block_data.get('transactions', [])\n            for tx in transactions:\n                if isinstance(tx, dict):  # Full transaction object\n                    await self.process_transaction(tx, block_number)\n                    block_stats['transactions'] += 1\n                    \n            # Get and process events/logs for this block\n            if self.config.get('processing.extract_logs', True):\n                events = await self.get_block_events(block_number)\n                if events:\n                    for event in events:\n                        await self.process_event(event, block_number)\n                        block_stats['events'] += 1\n                        \n        except Exception as e:\n            logger.error(f"Error processing block {block_number}: {e}")\n            \n        return block_stats\n        \n    async def process_transaction(self, tx_data: Dict[str, Any], block_number: int):\n        """Process a single transaction."""\n        try:\n            # Get transaction receipt for gas usage and status\n            receipt = await self.blockchain_client.get_transaction_receipt(tx_data['hash'])\n            \n            gas_used = None\n            status = "pending"\n            \n            if receipt:\n                gas_used = int(receipt.get('gasUsed', '0x0'), 16)\n                status = "success" if receipt.get('status') == '0x1' else "failed"\n                \n            # Store transaction data\n            if self.db_client:\n                self.db_client.write_transaction(tx_data, block_number, status, gas_used)\n                \n        except Exception as e:\n            logger.debug(f"Error processing transaction {tx_data.get('hash', 'unknown')}: {e}")\n            \n    async def get_block_events(self, block_number: int) -> Optional[List[Dict[str, Any]]]:\n        """Get all events/logs for a block."""\n        try:\n            return await self.blockchain_client.get_logs(block_number, block_number)\n        except Exception as e:\n            logger.debug(f"Error getting events for block {block_number}: {e}")\n            return None\n            \n    async def process_event(self, event_data: Dict[str, Any], block_number: int):\n        """Process a single event/log."""\n        try:\n            if self.db_client:\n                tx_hash = event_data.get('transactionHash', '')\n                self.db_client.write_event(event_data, block_number, tx_hash)\n        except Exception as e:\n            logger.debug(f"Error processing event: {e}")\n            \n    async def run_historical_processing(self) -> bool:\n        """Run the complete historical processing."""\n        logger.info("Starting historical blockchain data processing...")\n        \n        try:\n            # Initialize connections\n            if not await self.initialize():\n                return False\n                \n            # Determine processing range\n            start_block, end_block = await self.get_processing_range()\n            total_blocks = end_block - start_block + 1\n            \n            if total_blocks <= 0:\n                logger.info("No blocks to process (database is up to date)")\n                return True\n                \n            logger.info(\n                f"Processing blocks {start_block} to {end_block} ({total_blocks:,} blocks)"\n            )\n            \n            # Start processing with progress tracking\n            self.stats['start_time'] = time.time()\n            \n            with Progress(\n                TextColumn("[progress.description]{task.description}"),\n                BarColumn(),\n                "[progress.percentage]{task.percentage:>3.0f}%",\n                TimeRemainingColumn(),\n                console=console\n            ) as progress:\n                \n                task = progress.add_task(\n                    f"Processing {total_blocks:,} blocks...", \n                    total=total_blocks\n                )\n                \n                # Process in batches\n                current_block = start_block\n                \n                while current_block <= end_block:\n                    batch_end = min(current_block + self.batch_size - 1, end_block)\n                    \n                    # Process batch\n                    batch_stats = await self.process_block_batch(current_block, batch_end)\n                    \n                    # Update statistics\n                    self.stats['blocks_processed'] += batch_stats['blocks_processed']\n                    self.stats['transactions_processed'] += batch_stats['transactions_processed']\n                    self.stats['events_processed'] += batch_stats['events_processed']\n                    self.stats['errors'] += batch_stats['errors']\n                    \n                    # Update progress\n                    blocks_in_batch = batch_end - current_block + 1\n                    progress.update(task, advance=blocks_in_batch)\n                    \n                    # Update rate calculation\n                    elapsed = time.time() - self.stats['start_time']\n                    if elapsed > 0:\n                        self.stats['blocks_per_second'] = self.stats['blocks_processed'] / elapsed\n                        \n                    # Log progress\n                    if self.stats['blocks_processed'] % (self.batch_size * 10) == 0:\n                        logger.info(\n                            "Processing progress",\n                            blocks_processed=self.stats['blocks_processed'],\n                            transactions=self.stats['transactions_processed'],\n                            events=self.stats['events_processed'],\n                            rate_blocks_per_sec=f"{self.stats['blocks_per_second']:.2f}",\n                            current_block=batch_end\n                        )\n                        \n                    current_block = batch_end + 1\n                    \n            # Final statistics\n            total_time = time.time() - self.stats['start_time']\n            \n            logger.info(\n                "Historical processing completed",\n                total_blocks=self.stats['blocks_processed'],\n                total_transactions=self.stats['transactions_processed'],\n                total_events=self.stats['events_processed'],\n                total_errors=self.stats['errors'],\n                total_time_seconds=f"{total_time:.2f}",\n                avg_blocks_per_second=f"{self.stats['blocks_per_second']:.2f}"\n            )\n            \n            self.print_final_summary()\n            \n            return True\n            \n        except Exception as e:\n            logger.error(f"Historical processing failed: {e}")\n            return False\n        finally:\n            # Clean up connections\n            if self.blockchain_client:\n                self.blockchain_client.close()\n            if self.db_client:\n                self.db_client.close()\n                \n    def print_final_summary(self):\n        """Print a nice summary table."""\n        table = Table(title="üìä Historical Processing Summary")\n        table.add_column("Metric", style="cyan")\n        table.add_column("Value", style="green")\n        \n        total_time = time.time() - self.stats['start_time']\n        \n        table.add_row("Blocks Processed", f"{self.stats['blocks_processed']:,}")\n        table.add_row("Transactions Processed", f"{self.stats['transactions_processed']:,}")\n        table.add_row("Events Processed", f"{self.stats['events_processed']:,}")\n        table.add_row("Errors", str(self.stats['errors']))\n        table.add_row("Total Time", f"{total_time:.2f} seconds")\n        table.add_row("Average Rate", f"{self.stats['blocks_per_second']:.2f} blocks/sec")\n        \n        console.print(table)


async def main():\n    """Main function to run historical processing."""\n    # Setup logging\n    structlog.configure(\n        processors=[\n            structlog.processors.TimeStamper(fmt="iso"),\n            structlog.processors.add_log_level,\n            structlog.processors.JSONRenderer()\n        ],\n        wrapper_class=structlog.make_filtering_bound_logger(logging.INFO),\n        logger_factory=structlog.WriteLoggerFactory(),\n        cache_logger_on_first_use=True,\n    )\n    \n    # Load configuration\n    config = Config()\n    \n    # Create and run processor\n    processor = HistoricalProcessor(config)\n    success = await processor.run_historical_processing()\n    \n    if success:\n        console.print("\\n[bold green]‚úÖ Historical processing completed successfully![/bold green]")\n    else:\n        console.print("\\n[bold red]‚ùå Historical processing failed![/bold red]")\n        \n    return success


if __name__ == "__main__":\n    asyncio.run(main())